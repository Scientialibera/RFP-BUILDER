# RFP Builder - Enterprise Directory Structure

## Overview

The RFP Builder generates organized, enterprise-grade output structures for each run. Every execution creates a timestamped run directory with fully separated concerns across seven specialized subdirectories.

## Directory Hierarchy

```
backend/outputs/runs/
├── run_20260201_152345/              # Each run gets a timestamped directory
│   ├── word_document/                # Final deliverable
│   │   └── proposal.docx             # Generated DOCX with embedded visualizations
│   │
│   ├── image_assets/                 # Generated visualizations
│   │   ├── chart_001.png             # Seaborn/matplotlib outputs
│   │   ├── chart_002.png             # Generated by code execution
│   │   ├── histogram.png
│   │   └── ...
│   │
│   ├── diagrams/                     # Mermaid-generated diagrams
│   │   ├── architecture.svg
│   │   ├── timeline.svg
│   │   └── ...
│   │
│   ├── llm_interactions/             # Complete LLM API interaction logs
│   │   ├── analyze_analyze_rfp.json  # Function: analyze_rfp
│   │   ├── analyze_analyze_rfp.md    # Formatted markdown version
│   │   ├── generate_generate_rfp_response.json
│   │   ├── generate_generate_rfp_response.md
│   │   ├── plan_plan_proposal.json   # (if planner enabled)
│   │   ├── plan_plan_proposal.md
│   │   ├── critique_critique_response.json (if critiquer enabled)
│   │   └── critique_critique_response.md
│   │
│   ├── execution_logs/               # Code execution details
│   │   └── execution.json            # Success/failure status, errors, metrics
│   │
│   ├── metadata/                     # Structured data artifacts
│   │   ├── analysis.json             # RFP analysis (requirements, criteria, etc)
│   │   ├── plan.json                 # Section strategy (if planning enabled)
│   │   ├── critiques.json            # Feedback history (if critique enabled)
│   │   └── manifest.json             # Run summary and index
│   │
│   └── code_snapshots/               # Generated code evolution
│       ├── 01_initial_document_code.py
│       ├── 02_critique_revision_1.py
│       ├── 02_critique_revision_2.py
│       ├── 03_error_recovery_1.py
│       └── 99_final_document_code.py
│
├── run_20260201_152401/
│   └── [same structure as above]
│
└── [more run directories...]
```

## Subdirectory Purposes

### `word_document/`
**Purpose:** Final deliverable directory

- **proposal.docx** - The generated Microsoft Word document with all visualizations embedded
- Created when code execution succeeds
- Ready for delivery to clients
- Contains inline images and mermaid diagrams rendered as images

### `image_assets/`
**Purpose:** Generated chart and visualization files

- PNG files generated by seaborn/matplotlib during code execution
- Separated from word_document for reuse and alternative distribution
- Allows sending charts separately if needed
- Supports version control and archiving of visual assets

### `diagrams/`
**Purpose:** Mermaid diagram artifacts

- SVG files generated from mermaid code
- Includes: architectures, timelines, workflows, org charts, etc.
- Can be used in presentations or documentation
- Provides vector format for scaling

### `llm_interactions/`
**Purpose:** Complete LLM API interaction logs for auditing and debugging

- Named by: `{step_name}_{function_name}.json` and `.md`
- Contains:
  - Full function arguments sent to LLM
  - Raw LLM response before parsing
  - Parsed/validated results
  - Timestamps for each call
- Both JSON (machine-readable) and Markdown (human-readable) versions
- Useful for: debugging, audit trails, understanding decision rationale
- Shows every call to: analyze_rfp, plan_proposal, generate_rfp_response, critique_response

### `execution_logs/`
**Purpose:** Python code execution metrics and errors

- **execution.json** contains:
  - Document generation success/failure status
  - Error messages (if any)
  - Execution duration
  - Number of charts/diagrams generated
  - Number of error recovery attempts
  - Final metrics (lines of code, functions called, etc.)

### `metadata/`
**Purpose:** Structured data for analysis and auditing

#### `analysis.json`
- Extracted RFP requirements (from analyze_rfp function)
- Evaluation criteria the proposal will be scored on
- Deliverable specifications
- Schedule/timeline requirements
- Budget constraints (if mentioned)
- Any special requirements or scoring factors

#### `plan.json` *(if enable_planner=true)*
- High-level proposal strategy
- Proposed section structure
- Key themes and differentiators
- Budget allocation
- Timeline mapping
- Helps understand AI's reasoning before generation

#### `critiques.json` *(if enable_critiquer=true)*
- Array of critique feedback (if any)
- Issues found with generated code
- Suggestions for improvement
- Revision history
- Shows quality assurance iterations

#### `manifest.json`
- Run metadata summary
- Timestamp of execution
- Feature toggles used (planning, critique, error recovery)
- Number of sections/critiques
- Directory structure index

### `code_snapshots/`
**Purpose:** Generated python-docx code at each stage (for debugging/auditing)

- **01_initial_document_code.py** - First generation
- **02_critique_revision_N.py** - Post-critique revisions (numbered sequentially)
- **03_error_recovery_N.py** - Error recovery attempts (numbered sequentially)
- **99_final_document_code.py** - Final version used for document generation
- Shows exact python-docx code at each evolution stage
- Useful for: debugging generation issues, understanding revisions, code review

## Manifest File Example

```json
{
  "timestamp": "2026-02-01T09:37:45.896704",
  "run_dir": "outputs/runs/run_20260201_093448",
  "has_plan": true,
  "critique_count": 2,
  "subdirectories": {
    "word_document": "Final .docx proposal file",
    "image_assets": "Generated charts and visualizations",
    "diagrams": "Generated Mermaid diagrams",
    "llm_interactions": "LLM request/response logs",
    "execution_logs": "Code execution logs and errors",
    "metadata": "Analysis, plan, and critique JSON files",
    "code_snapshots": "Generated document code snapshots"
  }
}
```

## Use Cases

### Audit Trail
Navigate to `llm_interactions/` to see exactly what the LLM was told and what it responded with. Perfect for compliance and understanding decision-making.

### Debugging Failures
Check `execution_logs/execution.json` for errors, then look at `code_snapshots/` to see what code was generated. Compare with `llm_interactions/` to understand the generation process.

### Quality Review
Look at `metadata/critiques.json` to see what issues were found. Review `code_snapshots/02_critique_revision_*.py` to see improvements made.

### Visual Asset Distribution
All charts and diagrams are in `image_assets/` and `diagrams/` - easy to distribute separately or repurpose in other documents.

### Proposal Analysis
Open `metadata/analysis.json` to understand what was extracted from the RFP. Check `metadata/plan.json` to see the AI's strategy before generation.

## Navigation Tips

1. **Quick Start**: Open `word_document/proposal.docx` to see the final proposal
2. **Understanding Failures**: Check `execution_logs/execution.json` for error messages
3. **Audit Trail**: Open any file in `llm_interactions/` for complete LLM interaction history
4. **Proposal Strategy**: Read `metadata/plan.json` (if planning enabled) to understand the AI's approach
5. **Code Review**: Compare multiple files in `code_snapshots/` to see refinements
6. **Reusing Assets**: All charts are PNG files in `image_assets/`, all diagrams are SVG in `diagrams/`

## Configuration

Directory structure is controlled by these settings in `config.toml`:

```toml
[app]
output_dir = "./outputs/runs"        # Root runs directory (relative to backend/)

[workflow]
enable_planner = true                # Generates metadata/plan.json
enable_critiquer = true              # Generates metadata/critiques.json
max_critiques = 3                    # Number of critique iterations
max_error_loops = 2                  # Number of error recovery attempts
log_all_steps = true                 # Logs to llm_interactions/
```

## Storage Considerations

Each run generates approximately:
- 50KB - 500KB metadata files
- 100KB - 1MB LLM interaction logs
- 500KB - 5MB image assets (depending on chart complexity)
- 100KB - 500KB code snapshots
- **Total per run: 1MB - 10MB** (mostly images)

Clean old runs periodically if storage is limited:
```powershell
# Remove runs older than 30 days
Get-ChildItem "outputs/runs" | Where-Object {$_.LastWriteTime -lt (Get-Date).AddDays(-30)} | Remove-Item -Recurse
```

## Enterprise Features

✅ **Complete Traceability** - Every artifact organized and indexed
✅ **Audit Ready** - All LLM interactions logged in detail
✅ **Quality Assurance** - Critique history and code evolution tracked
✅ **Debugging Support** - Error logs, code snapshots, and execution metrics
✅ **Asset Management** - Images and diagrams separated for reuse
✅ **Metadata Rich** - Structured JSON for further analysis
✅ **Timestamped Runs** - Historical tracking by directory name
